{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bing翻译post方法抓包\n",
    "bing \n",
    "Request URL: https://cn.bing.com/ttranslatev3?isVertical=1&&IG=A45D80446A634A558A94AE850C68E671&IID=translator.5028.1\n",
    "Request Method: POST\n",
    "Status Code: 200 \n",
    "Remote Address: 202.89.233.101:443\n",
    "Referrer Policy: origin-when-cross-origin\n",
    "\n",
    "\n",
    "quest URL: https://cn.bing.com/ttranslatev3?isVertical=1&&IG=A45D80446A634A558A94AE850C68E671&IID=translator.5028.1\n",
    "Request Method: POST\n",
    "Status Code: 200 \n",
    "Remote Address: 202.89.233.101:443\n",
    "Referrer Policy: origin-when-cross-origin\n",
    "\n",
    "\n",
    "FROM DATA\n",
    "fromLang: auto-detect\n",
    "text: dhu\n",
    "to: zh-Hans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- encoding=UTF-8\n",
    "import re, json, random\n",
    "import requests\n",
    "from goose3 import Goose\n",
    "from goose3.text import StopWordsChinese #导入停用词\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参考\n",
    "[注释风格](https://zh-google-styleguide.readthedocs.io/en/latest/google-python-styleguide/python_style_rules/)\n",
    "[翻译api](https://juejin.im/post/5beaac9cf265da614a3a09a9)\n",
    "[goose提取正文](https://pypi.org/project/goose3/)\n",
    "[入门笔记](https://zhuanlan.zhihu.com/c_1231299003028840448)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可拓展范围：\n",
    "\n",
    "完善异常捕获\n",
    "\n",
    "目标字数 (调整gene_shuffle_article函数中para_num=5参数)\n",
    "\n",
    "使用更好的选择方式，比如使得开头段总是在前、结尾段总是在后\n",
    "\n",
    "选择中间语言（对使用不同的语言的目标用户泛化性能更强，同时需要调整停用词）\n",
    "\n",
    "混淆程度  （增加互译次数和翻译软件数）\n",
    "\n",
    "添加搜索和翻译引擎（百度、谷歌搜索，bing、deepl翻译）\n",
    "\n",
    "若干页搜索的解决（我没解决了）\n",
    "\n",
    "添加代理ip轮换？ （爬取频率应该不高，好像不是很需要）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_access_result(target_url=\"https://cn.bing.com/\", search_word=None):\n",
    "    \"\"\"\n",
    "        Args:\n",
    "            str:输入搜索字符串\n",
    "\n",
    "        Returns:\n",
    "            res:requests返回的response对象\n",
    "\n",
    "        Raises:\n",
    "            statuserror:爬取失败\n",
    "\n",
    "    \"\"\"\n",
    "    args={'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36'}\n",
    "    url=target_url\n",
    "    search={}\n",
    "    if search_word is not None:\n",
    "        search['q']=search_word\n",
    "    try:\n",
    "        res=requests.get(url, headers=args, params=search, timeout=10) #设置访问时间，超时便访问失败\n",
    "        res.raise_for_status()\n",
    "        res.encoding=res.apparent_encoding\n",
    "        # print(res.text[:1000])\n",
    "        return res\n",
    "    # except TimeoutError as tout:\n",
    "    #     print('Can\\'t access to website:'+target_url)\n",
    "    #     return None\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"spider failed\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Ciba:\n",
    "    '''使用金山词霸翻译\n",
    "        \n",
    "    Attributes:\n",
    "        url: 金山词霸post方法的url\n",
    "        headers: 报头参数\n",
    "\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        # self.word = word\n",
    "        self.url = 'http://fy.iciba.com/ajax.php?a=fy'\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) '\n",
    "                          'AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.132 Safari/537.36'\n",
    "        }\n",
    "        # 构造post请求的参数\n",
    "        # self.payload = {\n",
    "        #     'f': from_lang,\n",
    "        #     't': to_lang,\n",
    "        #     'w': self.word\n",
    "        # }\n",
    "\n",
    "    # 发送请求\n",
    "    def request_post(self, word, from_lang='auto', to_lang='auto'):\n",
    "        \"\"\"\n",
    "            Args:\n",
    "                word:翻译内容\n",
    "                from_lang：源语言\n",
    "                to_lang：目标语言\n",
    "\n",
    "            return：\n",
    "                None：访问失败\n",
    "                res.content.decode()：解码后的文本\n",
    "\n",
    "            Raises：\n",
    "                statuserror:爬取失败            \n",
    "        \"\"\"\n",
    "        payload = {\n",
    "            'f': from_lang,\n",
    "            't': to_lang,\n",
    "            'w': word\n",
    "        }\n",
    "        try:\n",
    "            res = requests.post(url=self.url, headers=self.headers, data=payload)\n",
    "            res.raise_for_status()\n",
    "            # print(res.content.decode())\n",
    "            return res.content.decode()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return None\n",
    "    # 解析数据\n",
    "    @staticmethod\n",
    "    def parse_data(data):\n",
    "        \"\"\"\n",
    "            Args：\n",
    "                data：爬虫解码后的文本\n",
    "\n",
    "            return：\n",
    "                dict_data['content']['out']:文字段翻译结果\n",
    "                dict_data['content']['word_mean']：词语翻译结果\n",
    "        \"\"\"\n",
    "        dict_data = json.loads(data)  #使用json解析\n",
    "        if 'out' in dict_data['content']:   #文字段翻译\n",
    "            return dict_data['content']['out']\n",
    "        elif 'word_mean' in dict_data['content']:  #词语翻译\n",
    "            return dict_data['content']['word_mean']\n",
    "\n",
    "    def translate(self, word, from_lang='auto', to_lang='auto'):\n",
    "        \"\"\"\n",
    "            Args:\n",
    "                word:翻译内容\n",
    "                from_lang：源语言\n",
    "                to_lang：目标语言\n",
    "\n",
    "            return：\n",
    "                None：访问失败\n",
    "                res.content.decode()：解码后的文本           \n",
    "        \"\"\"\n",
    "        data = self.request_post(word,from_lang,to_lang)\n",
    "        return self.parse_data(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bing_engine:\n",
    "    '''使用bing搜索\n",
    "        \n",
    "    Attributes:\n",
    "        url: bing搜索根目录url\n",
    "        headers: 报头参数\n",
    "\n",
    "\n",
    "    ''' \n",
    "\n",
    "    def __init__(self):\n",
    "        self.url=\"https://bing.com/\"\n",
    "        self.headers={'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36'}\n",
    "\n",
    "    def search(self, search_word=None):\n",
    "        \"\"\"\n",
    "            Args:\n",
    "                search_word:输入搜索字符串\n",
    "\n",
    "            Returns:\n",
    "                clean_res:返回一组搜索结果和链接\n",
    "\n",
    "            Raises:\n",
    "            statuserror:爬取失败\n",
    "\n",
    "        \"\"\"\n",
    "        search_para={}\n",
    "        if str is not None:\n",
    "            search_para['q']=search_word\n",
    "        try:\n",
    "            res=requests.get(url=self.url, headers=self.headers, params=search_para)\n",
    "            res.raise_for_status()\n",
    "            res.encoding=res.apparent_encoding\n",
    "            # print(res.text[:1000])\n",
    "            soup=BeautifulSoup(res.text, 'lxml')\n",
    "            clean_res=[]\n",
    "            for i in soup.findAll(name=['a'], target='_blank', text=re.compile('[^帮助]')):\n",
    "                # print(i.string,'\\n',i.attrs['href'])\n",
    "                clean_res.append((i.string, i.attrs['href']))\n",
    "            return clean_res\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"spider failed\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paragrams(search_res):\n",
    "    \"\"\"\n",
    "        TODO\n",
    "    \"\"\"\n",
    "    paras=[]\n",
    "    goose = Goose({'browser_user_agent': 'Mozilla', 'parser_class':'soup','stopwords_class': StopWordsChinese})  # 设置goose参数\n",
    "    for  ind, res_elem in enumerate(search_res):\n",
    "        res_herf=res_elem[1]\n",
    "        if get_access_result(target_url=res_herf)==None:  #测试是否可以访问\n",
    "            print('Can\\'t access to website:'+res_herf)\n",
    "            continue\n",
    "        article=goose.extract(url=res_herf)  #正文提取\n",
    "        paras.extend(list(article.cleaned_text.split()))  #分割成段\n",
    "    return paras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gene_shuffle_article(search_res, para_num=5):\n",
    "    \"\"\"\n",
    "        TODO\n",
    "    \"\"\"\n",
    "    paragrams=get_paragrams(search_res)  # 获取段落\n",
    "    # print(len(paragrams))\n",
    "    total=len(paragrams) # 总段落\n",
    "    article_gene=''\n",
    "    for i in random.sample(range(total),para_num):  #生成不重复的随机数\n",
    "        # print(i, '\\n' , paragrams[i])\n",
    "        article_gene+=paragrams[i]+'\\n'  # 选择若干段落拼接\n",
    "    return article_gene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rubbish_essay(search_word,search_engine='bing',trans_engine='ciba'):\n",
    "    \"\"\"\n",
    "        TODO\n",
    "    \"\"\"\n",
    "    if search_engine=='bing':\n",
    "        s_engine=bing_engine()\n",
    "    search_res=s_engine.search(search_word)\n",
    "    # print(search_res)\n",
    "    article_gene=gene_shuffle_article(search_res)\n",
    "    if trans_engine=='ciba':\n",
    "        t_engine=Ciba()\n",
    "    article_en=t_engine.translate(word=article_gene, from_lang='zh',to_lang='en')\n",
    "    article_final=t_engine.translate(word=article_en, from_lang='en',to_lang='zh')\n",
    "    return article_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "HTTPSConnectionPool(host='www.chinactp.org', port=443): Read timed out. (read timeout=10)\nspider failed\nCan't access to website:https://www.chinactp.org/wen/fanwen/392184.html\nHTTPSConnectionPool(host='www.chinactp.org', port=443): Read timed out. (read timeout=10)\nspider failed\nCan't access to website:https://www.chinactp.org/wen/fanwen/408095.html\n在今年的元旦期间，新的冠状病毒扰乱了我们的新年计划，甚至稀释了一年的味道。 随着疫情的发展和相关的保护措施、法规、文件等。 我成为了一名志愿者，作为一名志愿者，积极参与以上安排的志愿工作，不为钱，不为利，只有面对这一重大疫情贡献自己的微薄之力，使我国顺利度过难关..\n 瘟疫前，逆行可哭.. 预防战线，有太多的背影和故事让我们热泪盈眶.. 比如白衣天使们总是奋战在防疫第一线，有的为大家舍小家，有的日夜坚守岗位，有的把休息时间交给同事..比如职工自发集结，奋战72小时，让老楼变成新医院，以备救治和抢救需要；比如党员干部以冲锋队的姿态，保障群众的健康安全.. 党员带头当先锋，医务人员自愿，爱心人士积极参与.. 他们有的告别了刚断奶的孩子，有的离开了新婚夫妇，有的夫妻同时打架，有的全家人，有的累得直起腰来。 他们用辛勤的劳动和精湛的技术，筑牢了防疫防线！！ 抗击疫情的斗争还在继续，战争“疫情”还远未结束，对于一线“战士”来说，他们奋斗的样子值得铭记，奉献的精神值得致敬，疲惫的身心也需要细心的呵护！\n  “初心”耀耀，党旗飘扬，战斗堡垒连成一片，铸就“红色长城”。 面对疫情，在短时间内，国道进出口，高铁站出口和重要高速公路卡口悬挂党旗，交通堵塞防控的临时党支部如雨后春笋.. 同时，在乡村街道，村舍，居民楼，一处疫情防控巡逻临时党支部分散.. 一批党员志愿者带领群众参与了疫情防控宣传，重点人员处理，物资协调供应，后勤保障服务等工作.. 新型冠状病毒肺炎爆发以来，我们在全国各地看到，各级党员干部取消春节假期，跟踪安排，以支部为战斗堡垒，切实担负起各属地防控工作的重要责任，包户住户，落实各项防控措施，让党旗在“疫”线上迎风飘扬，成功带领广大群众在各党组织战斗堡垒作用下筑起“红墙”..\n 万物的冬天已经过去了，季节已经进入了温暖的春天，也是善良和理解的根源。 我们坚信，我们能战胜流行病，病后伤心，一定要春暖花开！！ 让我们一起为英雄加油，为我们的国家和城市，也为我们的普通人加油！！\n  预防和控制新的冠状动脉肺炎1000字5\n"
    }
   ],
   "source": [
    "print(rubbish_essay(search_word=\"抗疫感想\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('抗疫心得体会范文大全【10篇】', 'https://m.51test.net/show/9848999.html'),\n ('抗疫心得_抗击疫情心得体会范文6篇-理财-钱来也', 'https://www.qianlaiye.com/news/99718.html'),\n ('2020普通党员抗疫心得体会范文5篇', 'https://www.chinactp.org/wen/fanwen/392184.html'),\n ('2020青年抗疫心得体会_绽放战疫青春·坚定制度自信',\n  'http://m.xuexi.la/xindetihui/fanwen/263038.html'),\n ('抗疫心得体会', 'https://www.jxscct.com/fw/52724/'),\n ('抗疫心得体会1000字精选三篇', 'http://www.lizhigushi.com/renshengganwu/a21428.html'),\n ('抗疫志愿者的心得体会 - 心得体会 - 复来作文网', 'http://www.fulay.cn/html/105-176/176319.htm'),\n ('抗疫作文_感悟英雄在身边', 'https://www.chinactp.org/wen/fanwen/408095.html'),\n ('最新党员抗击疫情工作个人感悟_2020战疫工作心得体会_学习力',\n  'http://www.xuexili.com/xindetihui/3254.html'),\n ('新冠肺炎疫情防控心得1000字大全5篇_学习力', 'http://www.xuexili.com/xindetihui/3169.html')]"
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "source": [
    "'''\n",
    "test\n",
    "'''\n",
    "bing=bing_engine()\n",
    "bing.search(\"抗疫感想\") "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "0a32ad55-0a7c-4c2f-be72-91cbac48b6da",
   "display_name": "'Python Interactive'"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
