{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- encoding=UTF-8\n",
    "import re, json, random\n",
    "import requests \n",
    "from goose3 import Goose\n",
    "from goose3.text import StopWordsChinese #导入停用词\n",
    "from bs4 import BeautifulSoup\n",
    "from lxml import etree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参考\n",
    "[注释风格](https://zh-google-styleguide.readthedocs.io/en/latest/google-python-styleguide/python_style_rules/)\n",
    "[翻译api](https://juejin.im/post/5beaac9cf265da614a3a09a9)\n",
    "[goose提取正文](https://pypi.org/project/goose3/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可拓展范围：\n",
    "\n",
    "- [x] gui设计 hs\n",
    "\n",
    "- [x] 进度显示  hs\n",
    "\n",
    "- [x] 完善注释  wy\n",
    "\n",
    "- [x] 完善异常捕获  wy\n",
    "\n",
    "- [x] 目标字数 (调整gene_shuffle_article函数中para_num=5参数)  wy\n",
    "\n",
    "- 选择中间语言（判断unicode编码，同时需要调整停用词）  \n",
    "\n",
    "- [x] 混淆程度  （增加互译次数和翻译软件数,任意一种实现方式均可） rsy\n",
    "\n",
    "- [x] 添加搜索和翻译引擎（谷歌搜索等） rsy\n",
    "\n",
    "- 若干页搜索的解决（如果能实现，最好使用yield返回）受反爬虫技术影响，失败 rsy， wy \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_access_result(target_url=\"https://cn.bing.com/\", search_word=None):\n",
    "    \"\"\"\n",
    "        Args:\n",
    "            str:输入搜索字符串\n",
    "\n",
    "        Returns:\n",
    "            res:requests返回的response对象\n",
    "\n",
    "        Raises:\n",
    "            statuserror:爬取失败\n",
    "\n",
    "    \"\"\"\n",
    "    args={'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36'}\n",
    "    url=target_url\n",
    "    search={}\n",
    "    if search_word is not None:\n",
    "        search['q']=search_word\n",
    "    try:\n",
    "        res=requests.get(url, headers=args, params=search, timeout=10) #设置访问时间，超时便访问失败\n",
    "        res.raise_for_status()\n",
    "        res.encoding=res.apparent_encoding\n",
    "        # print(res.text[:1000])\n",
    "        return res\n",
    "    # except TimeoutError as tout:\n",
    "    #     print('Can\\'t access to website:'+target_url)\n",
    "    #     return None\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"spider failed\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Ciba:\n",
    "    '''使用金山词霸翻译\n",
    "        \n",
    "    Attributes:\n",
    "        url: 金山词霸post方法的url\n",
    "        headers: 报头参数\n",
    "\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        # self.word = word\n",
    "        self.url = 'http://fy.iciba.com/ajax.php?a=fy'\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) '\n",
    "                          'AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.132 Safari/537.36'\n",
    "        }\n",
    "        \n",
    "    # 发送请求\n",
    "    def request_post(self, word, from_lang='auto', to_lang='auto'):\n",
    "        \"\"\"\n",
    "            Args:\n",
    "                word:翻译内容\n",
    "                from_lang：源语言\n",
    "                to_lang：目标语言\n",
    "\n",
    "            return：\n",
    "                None：访问失败\n",
    "                res.content.decode()：解码后的文本\n",
    "\n",
    "            Raises：\n",
    "                statuserror:爬取失败            \n",
    "        \"\"\"\n",
    "        payload = {\n",
    "            'f': from_lang,\n",
    "            't': to_lang,\n",
    "            'w': word\n",
    "        }\n",
    "        try:\n",
    "            res = requests.post(url=self.url, headers=self.headers, data=payload)\n",
    "            res.raise_for_status()\n",
    "            # print(res.content.decode())\n",
    "            return res.content.decode()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return None\n",
    "    # 解析数据\n",
    "    @staticmethod\n",
    "    def parse_data(data):\n",
    "        \"\"\"\n",
    "            Args：\n",
    "                data：爬虫解码后的文本\n",
    "\n",
    "            return：\n",
    "                dict_data['content']['out']:文字段翻译结果\n",
    "                dict_data['content']['word_mean']：词语翻译结果\n",
    "        \"\"\"\n",
    "        dict_data = json.loads(data)  #使用json解析\n",
    "        if 'out' in dict_data['content']:   #文字段翻译\n",
    "            return dict_data['content']['out']\n",
    "        elif 'word_mean' in dict_data['content']:  #词语翻译\n",
    "            return dict_data['content']['word_mean']\n",
    "\n",
    "    def translate(self, word, from_lang='auto', to_lang='auto'):\n",
    "        \"\"\"\n",
    "            Args:\n",
    "                word:翻译内容\n",
    "                from_lang：源语言\n",
    "                to_lang：目标语言\n",
    "\n",
    "            return：\n",
    "                None：访问失败\n",
    "                res.content.decode()：解码后的文本           \n",
    "        \"\"\"\n",
    "        data = self.request_post(word,from_lang,to_lang)\n",
    "        if(data==None):\n",
    "            return ''\n",
    "        return self.parse_data(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bing:\n",
    "    '''使用金山词霸翻译\n",
    "        \n",
    "    Attributes:\n",
    "        url: bing翻译post方法的url\n",
    "        headers: 报头参数\n",
    "\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        # self.word = word\n",
    "        self.url = 'https://cn.bing.com/ttranslatev3?'\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) '\n",
    "                          'AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.132 Safari/537.36'\n",
    "        }\n",
    "\n",
    "\n",
    "    # 发送请求\n",
    "    def request_post(self, word, from_lang='auto-detect', to_lang='auto-detect'):\n",
    "        \"\"\"\n",
    "            Args:\n",
    "                word:翻译内容\n",
    "                from_lang：源语言\n",
    "                to_lang：目标语言\n",
    "\n",
    "            return：\n",
    "                None：访问失败\n",
    "                res.content.decode()：解码后的文本\n",
    "\n",
    "            Raises：\n",
    "                statuserror:爬取失败            \n",
    "        \"\"\"\n",
    "        if(from_lang=='zh'):\n",
    "            from_lang='zh-Hans'\n",
    "        if(to_lang=='zh'):\n",
    "            to_lang='zh-Hans'\n",
    "        data = {\n",
    "            'fromLang': from_lang,\n",
    "            'text': word,\n",
    "            'to': to_lang\n",
    "        }\n",
    "        try:\n",
    "            res = requests.post(url=self.url, headers=self.headers, data=data)\n",
    "            res.raise_for_status()\n",
    "            # print(res.content.decode())\n",
    "            return res.content.decode()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return None\n",
    "    # 解析数据\n",
    "    @staticmethod\n",
    "    def parse_data(data):\n",
    "        \"\"\"\n",
    "            Args：\n",
    "                data：爬虫解码后的文本\n",
    "\n",
    "            return：\n",
    "                dict_data[0]['translations'][0]['text']:翻译结果\n",
    "        \"\"\"\n",
    "        dict_data = json.loads(data)  #使用json解析\n",
    "        return dict_data[0]['translations'][0]['text']\n",
    "\n",
    "    def translate(self, word, from_lang='auto-detect', to_lang='auto-detect'):\n",
    "        \"\"\"\n",
    "            Args:\n",
    "                word:翻译内容\n",
    "                from_lang：源语言\n",
    "                to_lang：目标语言\n",
    "\n",
    "            return：\n",
    "                None：访问失败\n",
    "                res.content.decode()：解码后的文本           \n",
    "        \"\"\"\n",
    "        data = self.request_post(word,from_lang,to_lang)\n",
    "        if(data==None):\n",
    "            return ''\n",
    "        return self.parse_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bing_engine:\n",
    "    '''使用bing搜索\n",
    "        \n",
    "    Attributes:\n",
    "        url: bing搜索根目录url\n",
    "        headers: 报头参数\n",
    "\n",
    "    ''' \n",
    "\n",
    "    def __init__(self):\n",
    "        self.url=\"https://bing.com/\"\n",
    "        self.headers={'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36'}\n",
    "\n",
    "    def search(self, search_word=None):\n",
    "        \"\"\"\n",
    "            Args:\n",
    "                search_word:输入搜索字符串\n",
    "\n",
    "            Returns:\n",
    "                clean_res:返回一组搜索结果和链接\n",
    "\n",
    "            Raises:\n",
    "                statuserror:爬取失败\n",
    "\n",
    "        \"\"\"\n",
    "        search_para={}\n",
    "        if str is not None:\n",
    "            search_para['q']=search_word\n",
    "        try:\n",
    "            res=requests.get(url=self.url, headers=self.headers, params=search_para)\n",
    "            res.raise_for_status()\n",
    "            res.encoding=res.apparent_encoding\n",
    "            # print(res.text[:1000])\n",
    "            soup=BeautifulSoup(res.text, 'lxml')\n",
    "            clean_res=[]\n",
    "            for i in soup.findAll(name=['a'], target='_blank', text=re.compile('[^帮助]')):\n",
    "                # print(i.string,'\\n',i.attrs['href'])\n",
    "                clean_res.append((i.string, i.attrs['href']))\n",
    "            return clean_res\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"spider failed\")\n",
    "            return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class san60_engine:\n",
    "    '''使用360搜索\n",
    "    \n",
    "    Attributes:\n",
    "        url: 360搜索根目录url\n",
    "        headers: 报头参数\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.url=\"https://www.so.com/s\"\n",
    "        self.headers={'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.61 Safari/537.36'}\n",
    "        \n",
    "    def search(self, search_word=None):\n",
    "        \"\"\"\n",
    "            Args:\n",
    "                search_word:输入搜索字符串\n",
    "\n",
    "            Returns:\n",
    "                clean_res:返回一组搜索结果和链接\n",
    "\n",
    "            Raises:\n",
    "                statuserror:爬取失败\n",
    "        \"\"\"\n",
    "        search_para={}\n",
    "        if search_word is not None:\n",
    "            search_para['q']=search_word\n",
    "        try:\n",
    "            res=requests.get(url=self.url, headers=self.headers, params=search_para)\n",
    "            res.raise_for_status()\n",
    "            res.encoding=res.apparent_encoding\n",
    "            \n",
    "            tree = etree.HTML(res.text)\n",
    "            url_list=tree.xpath('//h3[@class]/a[@href]/@href')\n",
    "            for i in range(0,len(url_list)):\n",
    "                if 'link' in url_list[i]:\n",
    "                    try:\n",
    "                        tmp_url = requests.get(url_list[i])\n",
    "                        # print(tmp_url.text)\n",
    "                        patt = re.compile('replace(.*?)</script>')\n",
    "                        url_list[i]=eval(re.findall(patt,tmp_url.text)[0])\n",
    "                        # print(eval(re.findall(patt,tmp_url.text)[0]))\n",
    "                    except Exception as e:\n",
    "                        print(e)\n",
    "                        print(\"spider failed\")\n",
    "            content_list=tree.xpath('//h3[@class]/a[@href]/text()')\n",
    "            clean_res=list(zip(content_list[:-1],url_list[:-1]))\n",
    "            return clean_res\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"spider failed\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sogou_engine:\n",
    "    '''使用搜狗搜索\n",
    "    \n",
    "     Attributes:\n",
    "        url: bing搜索根目录url\n",
    "        headers: 报头参数\n",
    "    '''\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.url0=\"https://www.sogou.com\"\n",
    "        self.url=\"https://www.sogou.com/web\"\n",
    "        self.headers={'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.61 Safari/537.36'}\n",
    "        \n",
    "    \n",
    "    def search(self, search_word=None):\n",
    "        \"\"\"\n",
    "            Args:\n",
    "                search_word:输入搜索字符串\n",
    "\n",
    "            Returns:\n",
    "                clean_res:返回一组搜索结果和链接\n",
    "\n",
    "            Raises:\n",
    "                statuserror:爬取失败\n",
    "\n",
    "        \"\"\"\n",
    "        search_para={}\n",
    "        if search_word is not None:\n",
    "            search_para['query']=search_word\n",
    "        try:\n",
    "            res=requests.get(url=self.url, headers=self.headers, params=search_para)\n",
    "            res.raise_for_status()\n",
    "            res.encoding=res.apparent_encoding\n",
    "            tree=etree.HTML(res.text)\n",
    "            # soup=BeautifulSoup(res.text,'lxml')\n",
    "            # print(res.url)\n",
    "            # print(soup.prettify())\n",
    "            url_list=tree.xpath('//h3/a[@id and @target=\"_blank\" and @href]/@href')\n",
    "            for i in range(0,len(url_list)):\n",
    "                if 'link' in url_list[i]:\n",
    "                    url_list[i]=self.url0+url_list[i]\n",
    "                    try:\n",
    "                        tmp_url = requests.get(url_list[i])\n",
    "                        # print(tmp_url.text)\n",
    "                        patt = re.compile('replace(.*?)</script>')\n",
    "                        url_list[i]=eval(re.findall(patt,tmp_url.text)[0])\n",
    "                        # print(eval(re.findall(patt,tmp_url.text)[0]))\n",
    "                    except:\n",
    "                        pass\n",
    "            content_list=tree.xpath('//h3/a[@id and @target=\"_blank\" and @href]/text()')\n",
    "            clean_res=list(zip(content_list,url_list))\n",
    "            return clean_res\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"spider failed\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paragrams(search_res):\n",
    "    \"\"\"\n",
    "        Args:\n",
    "            search_res:返回一组搜索结果和链接\n",
    "\n",
    "        Returns:\n",
    "            clean_res:返回所有链接的正文段落\n",
    "\n",
    "        Raises:\n",
    "            e:文章段落分割异常\n",
    "    \"\"\"\n",
    "    paras=[]\n",
    "    goose = Goose({'browser_user_agent': 'Mozilla', 'parser_class':'soup','stopwords_class': StopWordsChinese})  # 设置goose参数\n",
    "    for  ind, res_elem in enumerate(search_res):\n",
    "        try:\n",
    "            res_herf=res_elem[1]\n",
    "            if get_access_result(target_url=res_herf)==None:  #测试是否可以访问\n",
    "                print('Can\\'t access to website:'+res_herf)\n",
    "                continue\n",
    "            article=goose.extract(url=res_herf)  #正文提取 异常处理\n",
    "            paras.extend(list(article.cleaned_text.split()))  #分割成段\n",
    "        except Exception as e:\n",
    "            print(\"Fail to split paragrams in\",res_elem[1], end='  ')\n",
    "            print(e)\n",
    "            continue\n",
    "    return paras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gene_shuffle_article(search_res, word_limit=800):\n",
    "    \"\"\"\n",
    "        Args:\n",
    "            search_res:返回一组搜索结果和链接\n",
    "            word_limit:文章最低字数限制\n",
    "        Returns:\n",
    "            article_gene:返回随机打乱后组合的文章\n",
    "\n",
    "        Raises:\n",
    "            e:段落组合失败\n",
    "\n",
    "    \"\"\"\n",
    "    paragrams=get_paragrams(search_res)  # 获取段落\n",
    "    # print(len(paragrams))\n",
    "    total_paras=len(paragrams) # 总段落\n",
    "    article_gene, total_words='', 0  ##生成文章与已加入段落数\n",
    "    st, total_st=set(), 0  ##使用过的段落集合\n",
    "    try:\n",
    "        while total_words<=word_limit and total_st<=0.2*total_paras:  #使用不超过20%的段落，字数超过后停止\n",
    "            i= random.randint(0,total_paras-1)  #生成不重复的随机数 \n",
    "            if(i not in st):  #不重复\n",
    "                st.add(i)   #插入集合\n",
    "                total_words+=len(paragrams[i])  ##总字数增加\n",
    "                total_st+=1 #总段落数增加\n",
    "                article_gene+=paragrams[i]+'\\n'  # 段落拼接\n",
    "            else:\n",
    "                continue #重复则略过\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    return article_gene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rubbish_essay(search_word, search_engine='bing', trans_engine='ciba', word_limit=800, confusion_level=1):\n",
    "    \"\"\"\n",
    "        Args:\n",
    "            search_word:搜索主题\n",
    "            search_engine: 搜索引擎\n",
    "            trans_engine: 翻译引擎\n",
    "            word_limit: 最少字数\n",
    "            confusion_level: 混淆程度\n",
    "        Returns:\n",
    "            warning: engine doesn't exist or word_limit is too large \n",
    "            article_gene:返回随机打乱后组合的文章\n",
    "\n",
    "    \"\"\"\n",
    "    if search_engine == 'bing':  #选择搜索引擎\n",
    "        s_engine = bing_engine()\n",
    "    elif search_engine=='360':\n",
    "        s_engine=san60_engine()\n",
    "    elif search_engine=='sogou':\n",
    "        s_engine=sogou_engine()\n",
    "    else :\n",
    "        return 'Please use a supported translation engine.'\n",
    "    search_res = s_engine.search(search_word) \n",
    "    article_gene = gene_shuffle_article(search_res, word_limit)\n",
    "    if trans_engine == 'ciba':  #选择翻译引擎\n",
    "        t_engine = Ciba()\n",
    "    elif trans_engine == 'bing':\n",
    "        t_engine = Bing()\n",
    "    else:\n",
    "        return 'Please use a supported translation engine.'\n",
    "    if len(article_gene)==0 :  #无搜索结果返回\n",
    "        return 'No result for {}'.format(search_word)\n",
    "    article_final = ''\n",
    "    for paragram in article_gene.split():  ## 按段落翻译，防止超字数限制\n",
    "        paragram_zh=paragram\n",
    "        while confusion_level:\n",
    "            confusion_level-=1\n",
    "            paragram_en = t_engine.translate(  \n",
    "                word=paragram_zh, from_lang='zh', to_lang='en')  \n",
    "            paragram_zh= t_engine.translate(\n",
    "                word=paragram_en, from_lang='en', to_lang='zh')\n",
    "        article_final +=paragram_zh+'\\n'\n",
    "    if len(article_final)<word_limit: #小于最小字长\n",
    "        article_final=\"The required number of words is too large.\"+article_final\n",
    "    return article_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "在特殊的“战场”上，党员干部要保持“责任田”的良好意图。 爆发是命令，防控是责任.. 目前冠状病毒诊断数量还在上升，比较危险的地方有党员干部的身影.. 一名党员是一面旗帜，要求党员干部首先要为人民群众的生命安全和健康负责，把防空工作作为当前最重要的工作来抓；要有责任心，在自己的岗位上谋政，尽职尽责，把防空的责任放在心里，抵制责任，确保人民群众的安全；把防控工作作为“不忘初心，牢记使命”主题教育的试金石，在战场上践行党的宗旨..\n目前，疫情还在发展，值此关键时刻，让勇敢的人们有后盾，让奉献的人们有后劲，让所有努力都奔向胜利的目标，我们还要继续相互支持、紧密配合、坚持到底。我们爱这个国家，爱这座城市，不仅在昨天和今天，更在未来的每一天。真爱和真情就是一种力量。“于你于我，同心同根”。这种力量来自你，来自我们中每一个履行公民义务的人！\n有目共睹的“中国速度”，筑牢打赢新冠肺炎疫情阻击战的信心。从1月24日到2月2日，中国在争分夺秒，与时间来了一场“拔河”比赛，短短十天，武汉火神山医院完工并交付使用;2月5日，雷神山医院也交付使用;2月3日到2月4日，武汉建立3所“方舱医院”;从发出应对新冠肺炎的倡议书起，1小时内收到400余份“请战书”;山东大学齐鲁医院紧急组建医疗队驰援湖北的通知发出后，1小时内迅速组建完毕……成立以钟南山院士为组长的新型冠状病毒感染的肺炎疫情联防联控工作机制科研攻关专家组，仅用1周时间完成病毒鉴定和测序。仅用2天时间建成信息报告系统。仅用3天时间初步研发出新冠病毒核酸检测试剂盒。由此可见，与时间赛跑，如此有目共睹的“中国速度”，何尝不能筑牢打赢战“疫”的信心。\n经过这次志愿者服务的经历，我深深的体会到党和国家以及人民群众之间相互配合的重要性。此外，我觉得自告奋勇，义无反顾应该是当代大学生应该发扬的品质，而不是抱着事不关己，高高挂起的心态。当国家遇到困难的时候，我们需要积极的参与进去，不逃避，不退缩，勇往直前。家喻户晓的是在这场战役中，我们每个人都奉献出了自己的微薄之力。俗话说，守得云开见月明，我相信只要我们众志成城，胜利终将会到来。加油，中国!加油，武汉!\n\n"
    }
   ],
   "source": [
    "print(rubbish_essay(search_word=\"抗疫感想\",word_limit=800))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "empty range for randrange() (0,0, 0)\nNo result for xakdslfja\n"
    }
   ],
   "source": [
    "print(rubbish_essay(search_word=\"xakdslfja\",word_limit=200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "HTTPConnectionPool(host='wenku.baidu.comhttps', port=80): Max retries exceeded with url: //wenku.baidu.com/ndcore/browse/sub?isType=10&fr=view (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000289F1140F28>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\nspider failed\nCan't access to website:http://wenku.baidu.comhttps://wenku.baidu.com/ndcore/browse/sub?isType=10&fr=view\nHTTPConnectionPool(host='wenku.baidu.comhttps', port=80): Max retries exceeded with url: //wenku.baidu.com/ndcore/browse/sub?isType=10&fr=view (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000289F11867B8>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\nspider failed\nCan't access to website:http://wenku.baidu.comhttps://wenku.baidu.com/ndcore/browse/sub?isType=10&fr=view\n\n通过对“青年大学习”网上主题团课第十期的学习，我了解到了国家对青少年的重视，从小时候的“我们是祖国的花朵”的兴奋，到现在“少年强则国强，少年富则国富”的责任，我们也在慢慢同国家响应着少年的成长。习主席说：“青年工作，抓住的是当下，传承的是根脉。”的确青少年承载的是一个国家的希望，我们应该共同去传承中国古时读书人的儒雅，武将的豪爽，这才是当代青年风采。然而国家也在不断向广大青少年提供展现自我的机会，为世人展示身为团员的我们的风度，并且团员是一个光荣的称号，所以我们应将自己最好的一面展现出来，因为我们是优秀的接班人。让我们一起在伟大的奋斗中实现自己的梦想吧!\n青春，是人生中最美的那一部分，是世界上任何一种语言中最动人的词汇之一。青年，是国家的未来和民族的希望，是社会上最富有朝气、最富有创造性、最富有生命力的群体。党的十九大以来，以习近平同志为核心的党中央站在党和国家事业发展薪火相传、后继有人的战略高度，关心青年的成长进步，为新时代党的青年工作指明前进方向。\n学习习近平总书记讲话，二是要多思考。在阅读大量材料的基础上，要不断深入思考，辨析各项政策、讲话推出的时代背景和时事背景，要学会对相关材料进行横向、纵向的比较和分析，从而掌握更明确的政策脉络，开阔自己的眼界视野和格局。\n青年强则国家强，青年行则国行。青年人有远大理想，有担当，国家才有希望。在这个中华民族伟大复兴的关键时刻，青年人的作为决定了国家的前途，毕竟青年人肩负着国家人民以及历史的重任。党中央对于青年人的关注也从未有过减少，正因为如此，青年人一代才有青年人的姿态，满怀理想，锐意进取，正因为如此，我们才比历史上任何一个时期更接近中华民族伟大复兴的目标。\n“青年兴则国家兴，青年强则国家强”，青年一代有理想、有本领、有担当，国家就有前途，民族就有希望。以下是小编为大家准备的共青团中央“青年大学习”团课学习心得，欢迎大家前来参阅。\n您的访问出错了\n学习习近平新时代中国特色社会主义思想和党的十九大精神，我懂得了：\n作为一名光荣的共青团员，更作为一名当代大学生，我们承担着坚持和发展中国特色社会主义、实现中华民族伟大复兴的中国梦的历史使命。在日常的学习生活中，主动学习马克思列宁主义，毛泽东思想，邓小平理论，“三个代表”重要思想，科学发展观，习近平系列讲话精神，并将其作为自己的行动指南。在学习和生活中率先起模范作用，积极努力成为一名优秀的共青团员，积极主动关注国家大事，关注时政，自觉践行社会主义核心价值观，学习十八大以来的重要会议精神。在生活中团结同学，自觉维护民族团结，国家统一。\n新时代青年应该读有字之书，读无字之书，青年当在知行合一中担当作为。在很大程度上，人类精神文明的成功是以书籍的形式保存下来的，读有字之书，正是将这些成果据为己有的过程；而另一部分，则需青年们在火热的实践锻炼和真刀真枪的摔打中获得。青年人要紧密结合新时代新实践，紧密结合思想和工作实际，多思多想、学深悟透，读懂有字之书，更要读通无字书，做起而行之的行动者、不做坐而论道的清谈客，当攻坚克难的奋斗者、不当怕见风雨的泥菩萨。\n\n"
    }
   ],
   "source": [
    "print(rubbish_essay(search_word=\"青年大学习感想\",word_limit=1200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[('16篇- 爱师网', 'https://www.is97.com/doc-174959-1.html'), ('范文10篇- 豆丁网', 'https://www.docin.com/p-2159284463.html'), ('感悟范文6篇- 道客巴巴', 'http://www.doc88.com/p-8149127255981.html'), ('| ', 'http://www.360kuai.com/pc/943d5a00e76bc2e97?cota=4&tj_url=so_rec&sign=360_57c3bbd1&refer_scene=so_1'), ('2018', 'https://www.docin.com/p-2129662326.html'), ('- 豆丁网', 'https://www.doc88.com/p-3009162559945.html'), ('2019范文- 道客巴巴', '//kch.so.com/result?kid=22&uiid=a8c3a57c3cd56bc6eb2fda5ab09ba8f4&q=%E9%9D%92%E5%B9%B4%E5%A4%A7%E5%AD%A6%E4%B9%A0%E6%84%9F%E6%83%B3&title=%E9%9D%92%E5%B9%B4%E5%85%9A%E5%91%98%E5%AD%A6%E4%B9%A0%E5%8D%81%E4%B9%9D%E5%A4%A7%E6%8A%A5%E5%91%8A%E5%BF%83%E5%BE%97%E4%BD%93%E4%BC%9A&category=education&srcg=kch_menu&src=kch_title'), ('党员', 'https://fanwen.chazidian.com/fanwen488730/'), ('十九', 'http://www.xuexila.com/yc/c179533.html'), ('报告', 'https://www.xuexila.com/yc/c396916.html')]\nBuilding prefix dict from the default dictionary ...\nLoading model from cache C:\\Users\\Lenovo\\AppData\\Local\\Temp\\jieba.cache\nLoading model cost 3.510 seconds.\nPrefix dict has been built successfully.\nHTTPConnectionPool(host='www.gkstk.com', port=80): Read timed out.\nspider failed\nCan't access to website:http://www.gkstk.com/wenku/2217366.html\nThe required number of words is too large.\n青年，是国家的未来和民族的希望，是社会上最富有朝气、最富有创造性、最富有生命力的群体，通过这次青年大学习我深刻的认识到作为一名共青团员，首先要端正思想，提高思想觉悟水平。在学习和生活上树立起作为共青团员就应起到带头和模范作用，其次用心参与学校组织的各项团员活动，在活动中学习理论，在活动中实践理论，这样才能做到学以致用，也贴合马克思的理论与实践相结合的原则。我们是新世纪的青年一代，我们是八九点钟的太阳，我们有国家领导人和全国人民对我们的殷切期望。我们要努力提高自身思想素质，弘扬社会主义道德风尚，认真遵守学院各项规章制度，自觉履行团员的义务，发挥团员的模范作用。\n这周通过对习近平新时代中国特色社会主义思想，和党的十九大精神的学习，让我明白文化是一个国家、一个民族的灵魂。文化兴国运兴，文化强民族强。没有高度的文化自信，没有文化的繁荣兴盛，就没有中华民族伟大复兴。要坚持中国特色社会主义文化发展道路，激发全民族文化创新创造活力，建设社会主义文化强国。中国特色社会主义文化，源自于中华民族五千多年文明历史所孕育的中华优秀传统文化，熔铸于党领导人民在革命、建设、改革中创造的革命文化和社会主义先进文化，植根于中国特色社会主义伟大实践。\n我知道，在新的历史条件下，深入学习和自觉学习党史，党章和三个代表的重要思想，对于巩固我们党的执政地位，把建设有中国特色社会主义事业不断推向前进具有十分重要的现实和长远意义。\n实现中华民族伟大复兴，是我们中国人民的伟大理想。没有无数先烈的英勇献身，就没有我们现在的美好生活，我们必须要高举中国特色社会主义伟大旗帜，为全面建成小康社会实现中国梦而奋斗。\n新时代的贵州精神，寄托着习近平总书记对贵州发展的殷切希望。新时代的号角，新征程的斗志昂扬。社会在进步，时代在发展，我们时刻都要牢记嘱托，感恩奋进，书写中华名族的伟大复兴中国梦的贵州新篇章，向党和人民交出一份满意的时代答卷。\n要不忘初心、坚定信念。当今社会物质生活越来越丰富多彩，同时面临的各种诱惑也越来越多，沉迷享乐、没有理想、没有追求，成为越来越多年轻人的标签。做有为青年，就要坚定理想信念，牢记青年人肩负的使命和责任，无论是家庭兴旺、社会繁荣还是国家昌盛，都离不开我们青年一代的拼搏奋斗。\n\n"
    }
   ],
   "source": [
    "print(rubbish_essay(search_word=\"青年大学习感想\",search_engine=\"sogou\",word_limit=1200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Building prefix dict from the default dictionary ...\nLoading model from cache C:\\Users\\Lenovo\\AppData\\Local\\Temp\\jieba.cache\nLoading model cost 3.247 seconds.\nPrefix dict has been built successfully.\nThe required number of words is too large.\n学习习近平总书记讲话，二是要多思考。在阅读大量材料的基础上，要不断深入思考，辨析各项政策、讲话推出的时代背景和时事背景，要学会对相关材料进行横向、纵向的比较和分析，从而掌握更明确的政策脉络，开阔自己的眼界视野和格局。\n作为一名团员，能够说这次的学习让我又一次正视了自我的团员身份，同时也如当头棒喝，提示着我就应重新审视团员形象和职责。\n习近平总书记特别强调：共青团必须站在理想信念的制高点上，引领青年树立理想。对于每一个人来说，信念都是很重要的，树立一个正确的理想信念，会使我们不在迷茫，会使我们充满动力。共青团是为青年工作的，所以共青团必须担起引领青年树立正确理想信念的重担。\n青年，是国家的未来和民族的希望，是社会上最富有朝气、最富有创造性、最富有生命力的群体。\n青年，是国家的未来和民族的希望，是社会上最富有朝气、最富有创造性、最富有生命力的群体，通过这次青年大学习我深刻的认识到作为一名共青团员，首先要端正思想，提高思想觉悟水平。在学习和生活上树立起作为共青团员就应起到带头和模范作用，其次用心参与学校组织的各项团员活动，在活动中学习理论，在活动中实践理论，这样才能做到学以致用，也贴合马克思的理论与实践相结合的原则。我们是新世纪的青年一代，我们是八九点钟的太阳，我们有国家领导人和全国人民对我们的殷切期望。我们要努力提高自身思想素质，弘扬社会主义道德风尚，认真遵守学院各项规章制度，自觉履行团员的义务，发挥团员的模范作用。\n大学生活是短暂和美丽的。我们不应虚度如此黄金岁月，不能让大学过得碌碌无为而遗憾一生。我们要着眼于自身实际，不断追求进步，明确自我读大学的目标，明确对自我的定位，明确要培养哪方面的潜力，明确要怎样提高自我的综合素质，明确学习的计划步骤，等等，逐项明确以后，并付诸行动中，才能使自我经过四年后有所得，我们必须时刻坚定我们的信念，努力学习，为广大团员青年做标榜，时刻谨记我们的职责，不忘初心，牢记使命，向前奋进。\n时代召唤青年，青年创造未来，通过学习主题团课，我更加明确自己肩负的责任，提高了政治觉悟和理论修养，我们应当加强自身建设，积极开展各项活动，认真落实各项任务，我们要始终站在时代发展的前列，应对挑战，务实创新，解放思想，实事求是，与时俱进，为实现中国梦而奋斗。\n我们要不断学习、不断奋斗、不断奉献，树立正确的理想信念，坚持自己的人生道路，为实现伟大中国梦而不懈努力。我此次参加三下乡活动和西北调研项目，亲自下乡了解农村现状、了解农民疾苦、与农民同吃同住，就是要同人民一起奋斗、同人民一起前进、同人民一起梦想，就是要不负习总书记的期望，为乡村振兴战略和三农工作的实施开展作奉献，为中国特色社会主义和全面小康社会的建设贡献自己的力量。\n习近平总书记充分肯定了我国民营经济的重要地位和作用，重申了我国坚持基本经济制度、坚持“两个毫不动摇”的一贯立场，作出了大力支持民营企业发展壮大的重要部署，在全面建设小康社会、进而全面建设社会主义现代化国家的征程中，民营经济只能壮大，不仅不能“离场”，更要发挥着推动新时代经济发展的作用。习总书记的讲话，给广大的民营企业家吃了一颗定心丸，让我们对近一段时间里纷纷扰扰的社会传言增加了免疫力，也极大地鼓舞和激励了广大企业家做好企业经营、为实现“中国梦”尽一份力量的信心与决心，使我们深刻感受到，国家和党中央一直在支持和关注着民营企业的发展态势，并为其进一步健康可持续发展修桥铺路。\n青春，是人生中最美的那一部分，是世界上任何一种语言中最动人的词汇之一。青年，是国家的未来和民族的希望，是社会上最富有朝气、最富有创造性、最富有生命力的群体。党的十九大以来，以习近平同志为核心的党中央站在党和国家事业发展薪火相传、后继有人的战略高度，关心青年的成长进步，为新时代党的青年工作指明前进方向。\n作为一名当代大学生，应该为自己确立好人生的目标、应该对自己的人生有一个导航。通过学习能更好、更合理地为自己的未来确立目标，更好地成长。列夫·托尔斯泰说：“理想是指路明灯，没有理想就没有坚定的方向，没有方向就没有奋斗目标”。因此，我应该建立崇高的理想，坚定的科学信念。但理想也不能脱离实际，基于对客观世界的认识和现实生活的了解。“看似寻常最奇崛，成如容易却艰辛。”这是习近平总书记对我们的期望，我们应当砥砺前行，发挥自己的价值。\n在政治课本中，我不止一次的看到了强调人们群众重要性的内容。而共青团是党领导的先进青年的群众组织，是党的助手和后备军，在党的群众工作格局中肩负着重点对青年群众开展工作的重要职责。综上所述，我深刻的意识到了要把大多数青年静静凝聚在共产党的周围，扩大党的执政基础的重要性。\n通过学习我更加深刻地认识到，习近平新时代中国特色社会主义思想是伟大旗帜、科学真理，其中关于青少年和共青团工作的一系列重要论述和指示熠熠生辉，是青年成长的指路明灯、共青团工作的根本遵循。我们要积极进取，散播正能量，做一名合格的大学生，更应该积极向党组织靠拢，争取早点成为一名优秀的共产党员。\n我意识到，我们作为新时代的新青年，应该时刻跟紧党的步伐，要有心中存党，行动为党，宣传爱党，永不叛党的思想觉悟。我们应该敢于有梦，勇于追梦，勤于圆梦。我们不仅要有志存高远的理想，也要有脚踏实地的肯干。只有真正的去现场实操，才能切实体会到自己工作的重要性。我们要不断学习、不断奋斗、不断奉献，树立正确的理想信念，坚持自己的人生道路，为实现伟大中国梦而不懈努力!\n中国共青团是共产党的后备军，二者是相相互关不行分裂的。开展增强共青团员认识主题教诲运动，是从源头上的确强化党的进步性建筑，稳固党执政的青年群众根本的重要步骤，是坚持党建带团建、团建促党建，增强共青团的吸引力、凝集力、创造力和战斗力的现实举动。我在这次对全团有重仔细义的教诲运动中也受益匪浅。\n说确实的，现如今大多数人都是团员，然则又有谁在待人接物的时辰是用精确的团员的职责感来要求自我的呢?在大众都懵懵懂懂仰视那一壁团旗，满怀期望准备投身中国共产主义青年团的时辰，必定对团的章程和团员的职责职守了若执掌，同时也想象着自我以后作为一个进步的团员，必定会用现实举动证明团的进步性和团员的主动热情。然则随着年纪增长，越来越多的人成为团员，入团成为初中弟子必修的课程。团员之间缺乏比力性和竞争性，自然在思维上会怠惰。\n“青年兴则国家兴，青年强则国家强。青年一代有理想、有本领、有担当，国家就有前途，民族就有希望。”这是习书记在党的报告中对新时代新青年提出的希望和要求。\n这周通过对习近平新时代中国特色社会主义思想，和党的十九大精神的学习，让我明白文化是一个国家、一个民族的灵魂。文化兴国运兴，文化强民族强。没有高度的文化自信，没有文化的繁荣兴盛，就没有中华民族伟大复兴。要坚持中国特色社会主义文化发展道路，激发全民族文化创新创造活力，建设社会主义文化强国。中国特色社会主义文化，源自于中华民族五千多年文明历史所孕育的中华优秀传统文化，熔铸于党领导人民在革命、建设、改革中创造的革命文化和社会主义先进文化，植根于中国特色社会主义伟大实践。\n作为一个新时代的青年人，应该时刻关注着党。在这次青年大学习中感受到了党对青年思想建设的肯定。共青团是一个培养新一代优秀青年的组织，我们就是祖国未来发展建设的栋梁。让我们坚定不移的走社会主义道路，做一个爱国，爱党，有理想，有抱负，有担当的人。为我们祖国日后的建设奉献自己的力量。\n“青年兴则国家兴，青年强则国家强”，青年一代有理想、有本领、有担当，国家就有前途，民族就有希望。“青年大学习”心得如何写呢?以下是小编为大家准备的2019“青年大学习”心得体会范文，欢迎大家前来参阅。\n作为当代青年，要自觉把个人的前途和国家的未来联系起来，要深刻的认识到，只有实现国家的富强，才会有支撑个人奋斗的舞台，只有实现社会的稳定，个人的幸福才能够得到保障。我们要不断学习，提高自己的思想觉悟和知识水平，增进对国情、党情的了解，增进对世界局势的认识。其中一个重要的切入点就是学习习近平总书记系列重要讲话，我们要学习讲话的精神，理解讲话的内涵，做到内化于心，外化于行。\n新时代学习新思想。要一如既往地坚持学习。学习是一个人成长进步的前提，是人生永恒的主题。\n用北宋大儒张横渠的话，与君共勉，为天地立心，为生民立命，为往圣继绝学，为万世开太平。要有立身为伞，为百姓遮风挡雨;俯身为牛，为百姓鞠躬尽瘁的精神。不忘初心，牢记使命，为实现中华民族伟大复兴的中国梦不懈奋斗。\n学习习近平总书记讲话，三是要坚持理论联系实际。在多阅读、多思考的基础上，我们要学会把学到的知识和心得应用到实际的工作和生活当中，让先进的思想武装自己头脑，实现学以致用。\n党的十九大报告中明确提出：“中国特色社会主义进入新时代，我国社会主要矛盾已经转化为人民日益增长的美好生活需要和不平衡不充分的发展之间的矛盾。”其一，我国稳定解决了十几亿人的温饱问题，总体上实现小康，不久将全面建成小康社会，人民美好生活需要日益广泛，不仅对物质文化生活提出了更高要求，而且在民主、法治、公平、正义、安全、等方面的要求日益增长。其二，我国社会生产力水平总体上显著提高，社会生产能力在很多方面进入世界前列，更加突出的问题是发展不平衡不充分，这已经成为满足人民日益增长的美好生活需要的主要制约因素。其三，我国社会主要矛盾的新变化是关系全局的历史性变化，对党和国家工作提出了许多新要求。\n\n"
    }
   ],
   "source": [
    "print(rubbish_essay(search_word=\"青年大学习感想\",search_engine=\"sogou\",word_limit=24534546))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "HTTPConnectionPool(host='list', port=80): Max retries exceeded with url: /108 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000023D3449DD30>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\nspider failed\nCan't access to website:http://jingyan.baidu.com//list/108\nHTTPConnectionPool(host='list', port=80): Max retries exceeded with url: /115 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000023D3449DDA0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\nspider failed\nCan't access to website:http://jingyan.baidu.com//list/115\n 名片解读：二号楼和一号楼，建于1956年，华中师范大学由谭华林迁至贵子山首批老楼之一，2013年被列为武汉市一级保护历史建筑.. 现为我校艺术学院办公教学楼..\n名片解读：华大图书馆的前身是创建于1903年的文华公书林，迄今已有百年的历史。1951年，由原华中大学、中华大学、中原大学教育学院三校的图书馆合并成为华中师范学院图书馆，1985年更名为华中师范大学图书馆。现在图书馆由总馆（新馆）和老馆组成。老馆建于1961年，面积为9000平方米，是华中师范大学从昙华林迁到桂子山上的首批老建筑之一。新馆2011年5月16日正式开放，总建筑面积约30357平方米，建有地上9层、地下2层，建筑高度约40米，可同时容纳5000人学习阅览。新馆和老馆中间由一条地下文化长廊连通，馆舍总面积达39，357平方米。\n：一楼，\n名片解读：田家炳教育书院建成于1998年，由香港田家炳基金会董事田庆先先生捐资兴建。地上10层，地下1层。现为教育学院、心理学院、学报、网络服务中心等单位办公用楼。\n对于在减肥的妹子，简直是在天堂和地狱间徘徊……\n今天再为大家介绍15所国内大学最好吃的大学食堂，啥也别说了，一起大饱眼福吧！\n食堂二楼的项目更丰富，手抓饼、广州糖水、拉面、猫耳面和刀削面都不错，不过重点推的是爆浆鸡排：\n我相信下一个一定是我们华夏民族的\n2.桂香园，传说中学校最好的一个食堂，在西区。但传说而已，毕竟个人的口味不同，不用太认真。桂香有三层，第一层主要经营一些早餐和面食；二楼菜品丰富，适合中午去吃；三楼主要为小宴，一般人比较少。\n\n"
    }
   ],
   "source": [
    "print(rubbish_essay(search_word=\"华中师范大学最好吃的食堂\",word_limit=600))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "0a32ad55-0a7c-4c2f-be72-91cbac48b6da",
   "display_name": "'Python Interactive'"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}